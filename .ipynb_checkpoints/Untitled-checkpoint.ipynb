{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating encoder:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                100384    \n",
      "=================================================================\n",
      "Total params: 119,584\n",
      "Trainable params: 119,392\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latentDim = 32\n",
    "encoderInput = Input(shape=(28, 28, 1)) \n",
    "\n",
    "X = Conv2D(32, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(encoderInput)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Conv2D(64, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "volumeSize = K.int_shape(X)\n",
    "X = Flatten()(X)\n",
    "X = Dense(latentDim)(X)\n",
    "\n",
    "encoder = Model(encoderInput, X, name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating decoder:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3136)              103488    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 159,553\n",
      "Trainable params: 159,361\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoderInput = Input(shape=(latentDim,))\n",
    "X = Dense(np.prod(volumeSize[1:]))(decoderInput)\n",
    "X = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(X)\n",
    "X = Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Conv2DTranspose(32, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Conv2DTranspose(1, (3, 3), padding=\"same\", activation=\"sigmoid\")(X)\n",
    "\n",
    "decoder = Model(decoderInput, X, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoder:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 32)                119584    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         159553    \n",
      "=================================================================\n",
      "Total params: 279,137\n",
      "Trainable params: 278,753\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(encoderInput, decoder(encoder(encoderInput)), name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test  = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# We'll put the 0 number as the anomaly here\n",
    "anomalies = np.where(y_train == 0)[0]\n",
    "valides = np.where(y_train != 0)[0]\n",
    "\n",
    "random.shuffle(anomalies)\n",
    "random.shuffle(valides)\n",
    "\n",
    "anomalies = anomalies[:int(len(anomalies) * 0.01)]\n",
    "\n",
    "anomalyImages = x_train[anomalies]\n",
    "valideImages  = x_train[valides]\n",
    "data = np.vstack([valideImages, anomalyImages])\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data = data.astype(\"float32\") / 255.0\n",
    "(trainX, testX) = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43308 samples, validate on 10828 samples\n",
      "Epoch 1/20\n",
      "43308/43308 [==============================] - 24s 556us/step - loss: 0.0090 - val_loss: 0.0054\n",
      "Epoch 2/20\n",
      "43308/43308 [==============================] - 23s 528us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 3/20\n",
      "43308/43308 [==============================] - 22s 503us/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 4/20\n",
      "43308/43308 [==============================] - 22s 508us/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 5/20\n",
      "43308/43308 [==============================] - 22s 510us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 6/20\n",
      "43308/43308 [==============================] - 23s 520us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 7/20\n",
      "43308/43308 [==============================] - 22s 510us/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 8/20\n",
      "43308/43308 [==============================] - 22s 508us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 9/20\n",
      "43308/43308 [==============================] - 22s 517us/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 10/20\n",
      "43308/43308 [==============================] - 23s 525us/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "43308/43308 [==============================] - 22s 512us/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "43308/43308 [==============================] - 21s 490us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "43308/43308 [==============================] - 21s 485us/step - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 14/20\n",
      "43308/43308 [==============================] - 21s 491us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "43308/43308 [==============================] - 22s 502us/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "43308/43308 [==============================] - 21s 494us/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "43308/43308 [==============================] - 21s 490us/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 18/20\n",
      "43308/43308 [==============================] - 21s 488us/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 19/20\n",
      "43308/43308 [==============================] - 21s 496us/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 20/20\n",
      "43308/43308 [==============================] - 22s 497us/step - loss: 0.0029 - val_loss: 0.0031\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "opt = Adam(lr= INIT_LR, decay = INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "history = autoencoder.fit(trainX, trainX, \n",
    "                         validation_data=(testX, testX),\n",
    "                         epochs=20,\n",
    "                         batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-c59e9bb4bb6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "test = encode_data(trainX)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABRUlEQVR4nGNgoA9gZGXGKaV+8M0eQyasckymd//8+3lcG5ssS8jzv3///Po1nw+LPvsX//59vf/i1xc9qACSPsMZIv9+fX2z8xmTA7q5TJZ3f/9+e3JJk2H4o5OijKju1Hnw99eb2abKvGxqV96FsaBIilz882OuAjMTAwOTxL2fC9lRHNr+50MyG8QM/vt/bwkiG6r14XcG1ChGwaN/nkkhSbJu+HOLA6ZQ8PDfR/xILhJ9/TsO7myDx78KkfzCaPTrmzCMw7/620lOZCvDft3lgbLZUz49UUcJgIo/x7khygSqPn9IhhoK96yAyM8/TCzKYVGyb+MP/kOWZORhENP4xmEVY8f5c0v+i/8owcPo+uPPh6vvfv35dd0IMyHwn/rz79+/vy8zhLHFs/KaN5+fdCqiSsECgpGRkfHfP2QJVIupCQAm8Xoy5bUnzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x23194012AC8>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.expand_dims(x_train[1], axis=0)\n",
    "image = (image / 255.0).astype(\"float32\")\n",
    "decodedImage = autoencoder.predict(image)\n",
    "decodedImage = np.squeeze((decodedImage * 255).astype(\"uint8\"))\n",
    "Image.fromarray(decodedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x23193F092B0>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = x_train[1]\n",
    "Image.fromarray(np.squeeze(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABI0lEQVR4nGNgwA0Y8chhAmZW3AbJHHzij8s0rtV//h7kxS7HkfHt35/LUtgNNXj27+/3KexYJbm3/Pn3e48wVjkmx+///j4wY8Lump1//33Lxu4VpuAP//5dEkf2CMIQ5hgeht9bPv3HKsmpw/T/2zlWZCtZ4CweTob/fwPE3x9+9hfDSkbld//+/Pj5+9frBpQggrjB6PW/v7///P379/dlDSaEnf8ZGBgYmLhY/v//8/H5o2+MWkv50Oz8z/Tn/78ve/Z8tY3n0jDcj+agR/8Ymfn17shYsTGwKzAi+4iBgYF73a9//369/fL7379fcRjOldnz69+/f//+/ft7hQ/TM/ylb/78+/fv+35ZZmRvQAGTZIS3wPUFR75jaKQ/wJo4ABtZdStzHut0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x23193D5FA58>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.expand_dims(x_train[36], axis=0)\n",
    "image = (image / 255.0).astype(\"float32\")\n",
    "decodedImage = autoencoder.predict(image)\n",
    "decodedImage = np.squeeze((decodedImage * 255).astype(\"uint8\"))\n",
    "Image.fromarray(decodedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0054638963,\n",
       " 0.0047725295,\n",
       " 0.002910885,\n",
       " 0.0009185389,\n",
       " 0.0024982456,\n",
       " 0.0029234358,\n",
       " 0.001048529,\n",
       " 0.002495231,\n",
       " 0.0006625155,\n",
       " 0.002619395,\n",
       " 0.0032521822,\n",
       " 0.0024855568,\n",
       " 0.0023506484,\n",
       " 0.0040051793,\n",
       " 0.00028996204,\n",
       " 0.001962979,\n",
       " 0.0044338005,\n",
       " 0.0028558054,\n",
       " 0.0015118793,\n",
       " 0.0018985914,\n",
       " 0.0030609618,\n",
       " 0.0062042,\n",
       " 0.0015906969,\n",
       " 0.00105324,\n",
       " 0.0032856374,\n",
       " 0.0040300647,\n",
       " 0.001351178,\n",
       " 0.004101675,\n",
       " 0.0032292325,\n",
       " 0.0015161334,\n",
       " 0.0058946684,\n",
       " 0.0032197225,\n",
       " 0.0015203439,\n",
       " 0.0017584204,\n",
       " 0.004460249,\n",
       " 0.0023896107,\n",
       " 0.0020364563,\n",
       " 0.006796736,\n",
       " 0.0016211251,\n",
       " 0.0030212018,\n",
       " 0.00049221923,\n",
       " 0.0059017194,\n",
       " 0.00062222866,\n",
       " 0.002018978,\n",
       " 0.0017551788,\n",
       " 0.001419725,\n",
       " 0.008608728,\n",
       " 0.0025929988,\n",
       " 0.0029280651,\n",
       " 0.0038502794,\n",
       " 0.0016497958,\n",
       " 0.004627783,\n",
       " 0.0022462674,\n",
       " 0.006023156,\n",
       " 0.002288886,\n",
       " 0.003585558,\n",
       " 0.004308743,\n",
       " 0.002260575,\n",
       " 0.0019512932,\n",
       " 0.0006587189,\n",
       " 0.0024009414,\n",
       " 0.0015144272,\n",
       " 0.0021072864,\n",
       " 0.008144282,\n",
       " 0.0023755978,\n",
       " 0.0016137476,\n",
       " 0.0019497798,\n",
       " 0.0065266294,\n",
       " 0.0069332616,\n",
       " 0.009673716,\n",
       " 0.0017727017,\n",
       " 0.0013227107,\n",
       " 0.00035067982,\n",
       " 0.0027286974,\n",
       " 0.0025176338,\n",
       " 0.0034085987,\n",
       " 0.0038288576,\n",
       " 0.0012109581,\n",
       " 0.0007823015,\n",
       " 0.001929582,\n",
       " 0.004206023,\n",
       " 0.0041490784,\n",
       " 0.0043048556,\n",
       " 0.0043050083,\n",
       " 0.0017883066,\n",
       " 0.005555829,\n",
       " 0.0027872068,\n",
       " 0.004455372,\n",
       " 0.007834777,\n",
       " 0.0036270497,\n",
       " 0.003037716,\n",
       " 0.002929881,\n",
       " 0.0016347307,\n",
       " 0.0031087233,\n",
       " 0.0015712873,\n",
       " 0.002866937,\n",
       " 0.0011049497,\n",
       " 0.005412003,\n",
       " 0.0010524136,\n",
       " 0.0011534597,\n",
       " 0.001825346,\n",
       " 0.0019534118,\n",
       " 0.00048263697,\n",
       " 0.0017857598,\n",
       " 0.000731961,\n",
       " 0.0019752206,\n",
       " 0.0017784825,\n",
       " 0.0029452462,\n",
       " 0.0058046957,\n",
       " 0.002691022,\n",
       " 0.0013339106,\n",
       " 0.008344059,\n",
       " 0.0007525824,\n",
       " 0.0015998115,\n",
       " 0.005583206,\n",
       " 0.0017272959,\n",
       " 0.0017712116,\n",
       " 0.0042374292,\n",
       " 0.0056508495,\n",
       " 0.005741829,\n",
       " 0.004184388,\n",
       " 0.0102500515,\n",
       " 0.0028968812,\n",
       " 0.0020176135,\n",
       " 0.0011527329,\n",
       " 0.005404159,\n",
       " 0.0015384868,\n",
       " 0.0029884388,\n",
       " 0.0010406668,\n",
       " 0.005149065,\n",
       " 0.0026938438,\n",
       " 0.0027508782,\n",
       " 0.0025178108,\n",
       " 0.0015398072,\n",
       " 0.0021370528,\n",
       " 0.0025292085,\n",
       " 0.00097556156,\n",
       " 0.0028407744,\n",
       " 0.001997383,\n",
       " 0.002550022,\n",
       " 0.001415534,\n",
       " 0.0016892339,\n",
       " 0.0015787169,\n",
       " 0.002488531,\n",
       " 0.0027246568,\n",
       " 0.0036202061,\n",
       " 0.003641199,\n",
       " 0.0019802407,\n",
       " 0.0026913614,\n",
       " 0.0025250108,\n",
       " 0.0023272634,\n",
       " 0.0021101865,\n",
       " 0.00040275193,\n",
       " 0.0011860381,\n",
       " 0.0034701196,\n",
       " 0.002854672,\n",
       " 0.0030204118,\n",
       " 0.0023940224,\n",
       " 0.0045278855,\n",
       " 0.003748126,\n",
       " 0.0031026097,\n",
       " 0.0029979816,\n",
       " 0.0013130034,\n",
       " 0.0025571915,\n",
       " 0.0022645043,\n",
       " 0.0018895513,\n",
       " 0.0010851618,\n",
       " 0.0022736592,\n",
       " 0.0014640898,\n",
       " 0.003934865,\n",
       " 0.0015728713,\n",
       " 0.0032079287,\n",
       " 0.0026933057,\n",
       " 0.0029053944,\n",
       " 0.0009559016,\n",
       " 0.003506133,\n",
       " 0.002065612,\n",
       " 0.0013578414,\n",
       " 0.0075528286,\n",
       " 0.0028317492,\n",
       " 0.0026140057,\n",
       " 0.0042610494,\n",
       " 0.0031483849,\n",
       " 0.0024969976,\n",
       " 0.0006962896,\n",
       " 0.003263705,\n",
       " 0.0027102572,\n",
       " 0.0051565683,\n",
       " 0.0029709218,\n",
       " 0.0036247317,\n",
       " 0.005912852,\n",
       " 0.0025935213,\n",
       " 0.0038547618,\n",
       " 0.0023657575,\n",
       " 0.0022037756,\n",
       " 0.0025490029,\n",
       " 0.0023126435,\n",
       " 0.0028908593,\n",
       " 0.0035396295,\n",
       " 0.003127246,\n",
       " 0.001067642,\n",
       " 0.0017334901,\n",
       " 0.0035783132,\n",
       " 0.0040047453,\n",
       " 0.0021328484,\n",
       " 0.0017509762,\n",
       " 0.0062993583,\n",
       " 0.0039927866,\n",
       " 0.0010153191,\n",
       " 0.003356002,\n",
       " 0.0023009,\n",
       " 0.0012573639,\n",
       " 0.0013732031,\n",
       " 0.0047985995,\n",
       " 0.0022627546,\n",
       " 0.002531562,\n",
       " 0.0026008212,\n",
       " 0.0018424505,\n",
       " 0.0020093478,\n",
       " 0.0028282076,\n",
       " 0.004198019,\n",
       " 0.0024773157,\n",
       " 0.003224341,\n",
       " 0.0015177558,\n",
       " 0.0055616414,\n",
       " 0.0033914247,\n",
       " 0.0011687786,\n",
       " 0.0020950781,\n",
       " 0.0045324876,\n",
       " 0.00430559,\n",
       " 0.0008206859,\n",
       " 0.0010466662,\n",
       " 0.004584892,\n",
       " 0.0019292444,\n",
       " 0.006508316,\n",
       " 0.0024885873,\n",
       " 0.0026156118,\n",
       " 0.002043699,\n",
       " 0.0020460319,\n",
       " 0.0018082515,\n",
       " 0.00559026,\n",
       " 0.002595311,\n",
       " 0.0044105593,\n",
       " 0.002018067,\n",
       " 0.005011777,\n",
       " 0.0036328577,\n",
       " 0.00984564,\n",
       " 0.0026624014,\n",
       " 0.0012662668,\n",
       " 0.004773375,\n",
       " 0.00590058,\n",
       " 0.0018523667,\n",
       " 0.0029487077,\n",
       " 0.0039196266,\n",
       " 0.0030519427,\n",
       " 0.004265598,\n",
       " 0.0013915732,\n",
       " 0.0023403522,\n",
       " 0.0011751456,\n",
       " 0.0033453929,\n",
       " 0.007364796,\n",
       " 0.0024223651,\n",
       " 0.004993932,\n",
       " 0.0019875297,\n",
       " 0.0027518186,\n",
       " 0.0033418895,\n",
       " 0.0050286176,\n",
       " 0.0014863524,\n",
       " 0.0029175363,\n",
       " 0.00063586887,\n",
       " 0.00039796217,\n",
       " 0.0019566654,\n",
       " 0.0031919174,\n",
       " 0.0026100925,\n",
       " 0.0026208747,\n",
       " 0.0018834883,\n",
       " 0.000613654,\n",
       " 0.0042370446,\n",
       " 0.0021628204,\n",
       " 0.002623546,\n",
       " 0.0016709338,\n",
       " 0.0034510307,\n",
       " 0.0020001817,\n",
       " 0.0059244805,\n",
       " 0.004195736,\n",
       " 0.0016683565,\n",
       " 0.0023174596,\n",
       " 0.001872074,\n",
       " 0.0012438168,\n",
       " 0.0013986136,\n",
       " 0.0007118461,\n",
       " 0.0018646896,\n",
       " 0.0018882185,\n",
       " 0.0066578225,\n",
       " 0.003912213,\n",
       " 0.0025407074,\n",
       " 0.0065998062,\n",
       " 0.0016600317,\n",
       " 0.0014853304,\n",
       " 0.004553925,\n",
       " 0.0016147599,\n",
       " 0.00271147,\n",
       " 0.0027125278,\n",
       " 0.003819559,\n",
       " 0.0022881755,\n",
       " 0.0024717308,\n",
       " 0.003212478,\n",
       " 0.001963265,\n",
       " 0.0035946455,\n",
       " 0.00070581114,\n",
       " 0.0006715456,\n",
       " 0.0014849147,\n",
       " 0.0068891095,\n",
       " 0.001237061,\n",
       " 0.0041515487,\n",
       " 0.0009753242,\n",
       " 0.001954179,\n",
       " 0.0037993577,\n",
       " 0.0040825102,\n",
       " 0.001311048,\n",
       " 0.004335851,\n",
       " 0.002613488,\n",
       " 0.0015945304,\n",
       " 0.0020317533,\n",
       " 0.002203393,\n",
       " 0.0038834466,\n",
       " 0.005891988,\n",
       " 0.0038459208,\n",
       " 0.0029832767,\n",
       " 0.0029949497,\n",
       " 0.0029278232,\n",
       " 0.0025792748,\n",
       " 0.002782439,\n",
       " 0.0035823085,\n",
       " 0.0047585885,\n",
       " 0.003107683,\n",
       " 0.0011806912,\n",
       " 0.002152519,\n",
       " 0.0019805823,\n",
       " 0.0047064456,\n",
       " 0.006879399,\n",
       " 0.0031690253,\n",
       " 0.0040756706,\n",
       " 0.0029694322,\n",
       " 0.0013773327,\n",
       " 0.0009913509,\n",
       " 0.002037097,\n",
       " 0.0031785376,\n",
       " 0.0024373883,\n",
       " 0.0029285154,\n",
       " 0.0011768218,\n",
       " 0.0023423382,\n",
       " 0.0044541634,\n",
       " 0.0011590602,\n",
       " 0.0014889808,\n",
       " 0.0012609063,\n",
       " 0.0020911098,\n",
       " 0.0010249813,\n",
       " 0.00089188135,\n",
       " 0.003928476,\n",
       " 0.00207669,\n",
       " 0.0028415527,\n",
       " 0.001560019,\n",
       " 0.002497433,\n",
       " 0.0015125953,\n",
       " 0.0022130373,\n",
       " 0.0005959755,\n",
       " 0.0016429441,\n",
       " 0.0036436578,\n",
       " 0.0014402776,\n",
       " 0.0014826352,\n",
       " 0.0017458142,\n",
       " 0.0021407905,\n",
       " 0.0015925356,\n",
       " 0.0021279464,\n",
       " 0.0037363209,\n",
       " 0.002285988,\n",
       " 0.002464702,\n",
       " 0.0040651564,\n",
       " 0.0018544169,\n",
       " 0.004455527,\n",
       " 0.0030183622,\n",
       " 0.0010716884,\n",
       " 0.0010456061,\n",
       " 0.003448057,\n",
       " 0.0050425623,\n",
       " 0.0045519006,\n",
       " 0.0014432322,\n",
       " 0.0023867823,\n",
       " 0.0009548433,\n",
       " 0.006530748,\n",
       " 0.00817922,\n",
       " 0.003537184,\n",
       " 0.0066205226,\n",
       " 0.0008457919,\n",
       " 0.0017209694,\n",
       " 0.0033741277,\n",
       " 0.0008723502,\n",
       " 0.00079444505,\n",
       " 0.003482292,\n",
       " 0.0042998297,\n",
       " 0.0020372032,\n",
       " 0.0013420894,\n",
       " 0.0031427562,\n",
       " 0.010015726,\n",
       " 0.0023842754,\n",
       " 0.0007676992,\n",
       " 0.0033709724,\n",
       " 0.00053027394,\n",
       " 0.004070914,\n",
       " 0.0024082402,\n",
       " 0.0020107217,\n",
       " 0.0019511213,\n",
       " 0.0018921988,\n",
       " 0.0018993467,\n",
       " 0.008705496,\n",
       " 0.0014432528,\n",
       " 0.00081625185,\n",
       " 0.00532092,\n",
       " 0.0017451516,\n",
       " 0.0012250501,\n",
       " 0.0024853465,\n",
       " 0.0012522306,\n",
       " 0.001245083,\n",
       " 0.0034349822,\n",
       " 0.0034768393,\n",
       " 0.0017605731,\n",
       " 0.003745361,\n",
       " 0.0013959034,\n",
       " 0.003347724,\n",
       " 0.0015658152,\n",
       " 0.0017547851,\n",
       " 0.0033923157,\n",
       " 0.0025048754,\n",
       " 0.0033518625,\n",
       " 0.0031613337,\n",
       " 0.0012929172,\n",
       " 0.002681625,\n",
       " 0.0024558692,\n",
       " 0.004150868,\n",
       " 0.011769173,\n",
       " 0.0014124026,\n",
       " 0.0024815537,\n",
       " 0.0017068912,\n",
       " 0.0024206704,\n",
       " 0.0026858237,\n",
       " 0.0032534075,\n",
       " 0.0017240604,\n",
       " 0.0019836507,\n",
       " 0.0022110264,\n",
       " 0.00095914386,\n",
       " 0.008396022,\n",
       " 0.00374292,\n",
       " 0.0031668984,\n",
       " 0.00049359305,\n",
       " 0.0009602284,\n",
       " 0.0030719833,\n",
       " 0.003178496,\n",
       " 0.0038097105,\n",
       " 0.0022475875,\n",
       " 0.0054826136,\n",
       " 0.0017486215,\n",
       " 0.009656968,\n",
       " 0.001459022,\n",
       " 0.0044253436,\n",
       " 0.0023955288,\n",
       " 0.0006248111,\n",
       " 0.0023445117,\n",
       " 0.0021929217,\n",
       " 0.0046002623,\n",
       " 0.0010672222,\n",
       " 0.0013601827,\n",
       " 0.0023118781,\n",
       " 0.0034613335,\n",
       " 0.0032216278,\n",
       " 0.0005969001,\n",
       " 0.0030521792,\n",
       " 0.003030062,\n",
       " 0.00266002,\n",
       " 0.0014101628,\n",
       " 0.002809653,\n",
       " 0.002656931,\n",
       " 0.0013644731,\n",
       " 0.0024404852,\n",
       " 0.0006564867,\n",
       " 0.0033074697,\n",
       " 0.0029606211,\n",
       " 0.0034630024,\n",
       " 0.0037610542,\n",
       " 0.0051164394,\n",
       " 0.0023726241,\n",
       " 0.0013047697,\n",
       " 0.0035683264,\n",
       " 0.0034370802,\n",
       " 0.0041806344,\n",
       " 0.001975783,\n",
       " 0.0030050867,\n",
       " 0.0010510697,\n",
       " 0.0034099205,\n",
       " 0.003831749,\n",
       " 0.0035247074,\n",
       " 0.0018700605,\n",
       " 0.0014666213,\n",
       " 0.002807196,\n",
       " 0.0039069764,\n",
       " 0.0010049039,\n",
       " 0.0066515286,\n",
       " 0.00050142093,\n",
       " 0.0007126334,\n",
       " 0.0045430725,\n",
       " 0.00066159194,\n",
       " 0.0014605267,\n",
       " 0.0024454922,\n",
       " 0.0026228358,\n",
       " 0.002226902,\n",
       " 0.0018434112,\n",
       " 0.0044952836,\n",
       " 0.0058254534,\n",
       " 0.0038307183,\n",
       " 0.003414943,\n",
       " 0.0011615903,\n",
       " 0.0021449279,\n",
       " 0.0016675004,\n",
       " 0.0017411804,\n",
       " 0.0067464793,\n",
       " 0.0044067143,\n",
       " 0.0055837757,\n",
       " 0.005476953,\n",
       " 0.004087821,\n",
       " 0.0020875381,\n",
       " 0.0027470512,\n",
       " 0.0047811884,\n",
       " 0.003767906,\n",
       " 0.0006697864,\n",
       " 0.0014383273,\n",
       " 0.0015385564,\n",
       " 0.006912442,\n",
       " 0.0030623574,\n",
       " 0.001263831,\n",
       " 0.0047439868,\n",
       " 0.0032002071,\n",
       " 0.0030725654,\n",
       " 0.0059073493,\n",
       " 0.001617882,\n",
       " 0.0023497136,\n",
       " 0.0035110114,\n",
       " 0.0030733757,\n",
       " 0.002398778,\n",
       " 0.0029186758,\n",
       " 0.00359538,\n",
       " 0.001269814,\n",
       " 0.00852482,\n",
       " 0.0011636231,\n",
       " 0.0025546777,\n",
       " 0.0047107474,\n",
       " 0.0031558706,\n",
       " 0.0005565231,\n",
       " 0.002962583,\n",
       " 0.0038329521,\n",
       " 0.00312682,\n",
       " 0.00057199824,\n",
       " 0.004456524,\n",
       " 0.0015207252,\n",
       " 0.008455302,\n",
       " 0.0016557216,\n",
       " 0.0059073763,\n",
       " 0.0014531048,\n",
       " 0.0010261171,\n",
       " 0.0031350334,\n",
       " 0.0025435295,\n",
       " 0.0027694462,\n",
       " 0.002881069,\n",
       " 0.0012472485,\n",
       " 0.0008327152,\n",
       " 0.0012795891,\n",
       " 0.0040701465,\n",
       " 0.0018072147,\n",
       " 0.0053148246,\n",
       " 0.002372733,\n",
       " 0.0042518144,\n",
       " 0.0019151719,\n",
       " 0.0020813188,\n",
       " 0.0026334163,\n",
       " 0.0012869801,\n",
       " 0.0046618786,\n",
       " 0.0020142887,\n",
       " 0.0012612675,\n",
       " 0.0011435569,\n",
       " 0.0021815584,\n",
       " 0.0019503133,\n",
       " 0.0024538762,\n",
       " 0.0032435409,\n",
       " 0.0032562637,\n",
       " 0.00060689886,\n",
       " 0.0044908547,\n",
       " 0.003144457,\n",
       " 0.0037807645,\n",
       " 0.004985837,\n",
       " 0.0056424798,\n",
       " 0.002110904,\n",
       " 0.0008033263,\n",
       " 0.0021946603,\n",
       " 0.0021634125,\n",
       " 0.0049413084,\n",
       " 0.000454734,\n",
       " 0.005961832,\n",
       " 0.0037554721,\n",
       " 0.00413794,\n",
       " 0.0015777103,\n",
       " 0.000941832,\n",
       " 0.0040895217,\n",
       " 0.0010769154,\n",
       " 0.0031788864,\n",
       " 0.0023233183,\n",
       " 0.0020435906,\n",
       " 0.0051205643,\n",
       " 0.0015545243,\n",
       " 0.0016920822,\n",
       " 0.0009385037,\n",
       " 0.0028445118,\n",
       " 0.0044096033,\n",
       " 0.0028807768,\n",
       " 0.0033822497,\n",
       " 0.0025456415,\n",
       " 0.002356323,\n",
       " 0.0012398569,\n",
       " 0.00326218,\n",
       " 0.004840612,\n",
       " 0.005134356,\n",
       " 0.0033512851,\n",
       " 0.00191519,\n",
       " 0.0022665411,\n",
       " 0.0019356102,\n",
       " 0.008146016,\n",
       " 0.00073632743,\n",
       " 0.011504353,\n",
       " 0.001334091,\n",
       " 0.0012225101,\n",
       " 0.00064277725,\n",
       " 0.0037632086,\n",
       " 0.0024366193,\n",
       " 0.0027034061,\n",
       " 0.005798719,\n",
       " 0.0021825891,\n",
       " 0.00090205215,\n",
       " 0.0027573463,\n",
       " 0.001321112,\n",
       " 0.0035042705,\n",
       " 0.00063630525,\n",
       " 0.00287081,\n",
       " 0.0021992687,\n",
       " 0.0010163005,\n",
       " 0.0014856935,\n",
       " 0.0028991268,\n",
       " 0.0014997789,\n",
       " 0.003524771,\n",
       " 0.0028242578,\n",
       " 0.0018202453,\n",
       " 0.0062314635,\n",
       " 0.0034578254,\n",
       " 0.001110496,\n",
       " 0.0029630016,\n",
       " 0.00415171,\n",
       " 0.002574886,\n",
       " 0.0032465963,\n",
       " 0.0052951737,\n",
       " 0.002253459,\n",
       " 0.004504341,\n",
       " 0.0028340956,\n",
       " 0.0026244498,\n",
       " 0.011165548,\n",
       " 0.0016720118,\n",
       " 0.0012993973,\n",
       " 0.0027155413,\n",
       " 0.0041316454,\n",
       " 0.0015160072,\n",
       " 0.0009169472,\n",
       " 0.0022465102,\n",
       " 0.00055595435,\n",
       " 0.0034543087,\n",
       " 0.0018633804,\n",
       " 0.0018671928,\n",
       " 0.0021936842,\n",
       " 0.00090512494,\n",
       " 0.0019557474,\n",
       " 0.001519767,\n",
       " 0.0013803496,\n",
       " 0.0017847639,\n",
       " 0.0017041925,\n",
       " 0.003127817,\n",
       " 0.0027290669,\n",
       " 0.0011916697,\n",
       " 0.004481077,\n",
       " 0.0033869506,\n",
       " 0.005078161,\n",
       " 0.0015303378,\n",
       " 0.0020590809,\n",
       " 0.004034505,\n",
       " 0.0015702914,\n",
       " 0.003171916,\n",
       " 0.004361037,\n",
       " 0.0019707736,\n",
       " 0.0029353327,\n",
       " 0.0011723805,\n",
       " 0.006839488,\n",
       " 0.002808892,\n",
       " 0.0022870805,\n",
       " 0.001929791,\n",
       " 0.0023702087,\n",
       " 0.0048144795,\n",
       " 0.00071474025,\n",
       " 0.0013837307,\n",
       " 0.00575283,\n",
       " 0.0029465922,\n",
       " 0.0029480355,\n",
       " 0.002189248,\n",
       " 0.005612334,\n",
       " 0.003374146,\n",
       " 0.0014414018,\n",
       " 0.002369697,\n",
       " 0.003310329,\n",
       " 0.0026109887,\n",
       " 0.003306045,\n",
       " 0.001118073,\n",
       " 0.00084216543,\n",
       " 0.0031118048,\n",
       " 0.0028645736,\n",
       " 0.0018302968,\n",
       " 0.0008650059,\n",
       " 0.004057007,\n",
       " 0.0031898431,\n",
       " 0.0012239491,\n",
       " 0.0023191432,\n",
       " 0.003353347,\n",
       " 0.0033739598,\n",
       " 0.0014329179,\n",
       " 0.0044235378,\n",
       " 0.0034583234,\n",
       " 0.00056283915,\n",
       " 0.0029129994,\n",
       " 0.002392471,\n",
       " 0.0018532759,\n",
       " 0.0023174747,\n",
       " 0.0032150457,\n",
       " 0.004534257,\n",
       " 0.003492579,\n",
       " 0.0018861266,\n",
       " 0.00088335376,\n",
       " 0.0042330986,\n",
       " 0.0038236221,\n",
       " 0.0018975367,\n",
       " 0.0024278867,\n",
       " 0.0019253265,\n",
       " 0.0014384015,\n",
       " 0.0023710113,\n",
       " 0.0020719974,\n",
       " 0.008971663,\n",
       " 0.002521469,\n",
       " 0.0031116228,\n",
       " 0.0033520022,\n",
       " 0.0047945958,\n",
       " 0.0028963168,\n",
       " 0.0028589438,\n",
       " 0.0015971215,\n",
       " 0.0020359515,\n",
       " 0.0006599414,\n",
       " 0.0018095758,\n",
       " 0.0013364275,\n",
       " 0.0020018697,\n",
       " 0.0040844437,\n",
       " 0.0057511623,\n",
       " 0.0019565332,\n",
       " 0.0039584194,\n",
       " 0.00087170774,\n",
       " 0.003615105,\n",
       " 0.010247065,\n",
       " 0.002713114,\n",
       " 0.002392049,\n",
       " 0.0058308197,\n",
       " 0.0011776012,\n",
       " 0.000643047,\n",
       " 0.006076906,\n",
       " 0.0020259172,\n",
       " 0.00070893666,\n",
       " 0.0053366525,\n",
       " 0.0013510437,\n",
       " 0.00091474876,\n",
       " 0.0037443896,\n",
       " 0.002117905,\n",
       " 0.0014109528,\n",
       " 0.0035316963,\n",
       " 0.0015326194,\n",
       " 0.0055303015,\n",
       " 0.0027353552,\n",
       " 0.0029182653,\n",
       " 0.0016886379,\n",
       " 0.0040178094,\n",
       " 0.0019690448,\n",
       " 0.0009660885,\n",
       " 0.0015223472,\n",
       " 0.001715599,\n",
       " 0.0020500768,\n",
       " 0.0021101239,\n",
       " 0.0013223828,\n",
       " 0.0015536273,\n",
       " 0.0017802309,\n",
       " 0.0021656554,\n",
       " 0.0024174459,\n",
       " 0.0015192116,\n",
       " 0.0006475488,\n",
       " 0.0013722724,\n",
       " 0.0018571393,\n",
       " 0.0016495198,\n",
       " 0.0012042982,\n",
       " 0.0055807494,\n",
       " 0.0013984684,\n",
       " 0.0019093947,\n",
       " 0.0016458351,\n",
       " 0.0032884507,\n",
       " 0.0019931167,\n",
       " 0.0020037624,\n",
       " 0.0012376785,\n",
       " 0.0024213672,\n",
       " 0.0012782345,\n",
       " 0.0011779935,\n",
       " 0.0042526703,\n",
       " 0.008194901,\n",
       " 0.0016055192,\n",
       " 0.0054485383,\n",
       " 0.0016743661,\n",
       " 0.0019158308,\n",
       " 0.0032165956,\n",
       " 0.0019731899,\n",
       " 0.0007957167,\n",
       " 0.0026150527,\n",
       " 0.00096491433,\n",
       " 0.003622934,\n",
       " 0.002560252,\n",
       " 0.00048172154,\n",
       " 0.0009380606,\n",
       " 0.0022406308,\n",
       " 0.001913983,\n",
       " 0.0030139145,\n",
       " 0.002709783,\n",
       " 0.0057941796,\n",
       " 0.0021620009,\n",
       " 0.003878251,\n",
       " 0.0038162568,\n",
       " 0.002890459,\n",
       " 0.0037823971,\n",
       " 0.003155445,\n",
       " 0.0021152284,\n",
       " 0.0010323353,\n",
       " 0.0008317808,\n",
       " 0.003403036,\n",
       " 0.003643143,\n",
       " 0.0012880301,\n",
       " 0.0034946185,\n",
       " 0.0026756506,\n",
       " 0.0032383155,\n",
       " 0.0029643343,\n",
       " 0.0021807346,\n",
       " 0.0038473757,\n",
       " 0.0016156854,\n",
       " 0.0013538733,\n",
       " 0.002233815,\n",
       " 0.0077378685,\n",
       " 0.0018895417,\n",
       " 0.0019834533,\n",
       " 0.0055713314,\n",
       " 0.0027438214,\n",
       " 0.0003944428,\n",
       " 0.004103794,\n",
       " 0.00294376,\n",
       " 0.0044131284,\n",
       " 0.004030704,\n",
       " 0.0022437784,\n",
       " 0.0015962024,\n",
       " 0.0018322067,\n",
       " 0.0025385767,\n",
       " 0.0013005524,\n",
       " 0.0024991932,\n",
       " 0.0024155034,\n",
       " 0.0010553123,\n",
       " 0.0029069532,\n",
       " 0.0021571238,\n",
       " 0.0049618594,\n",
       " 0.0012018296,\n",
       " 0.0011221159,\n",
       " 0.0025198143,\n",
       " 0.0029906905,\n",
       " 0.0004356331,\n",
       " 0.0036882563,\n",
       " 0.003713573,\n",
       " 0.0016928698,\n",
       " 0.0032445248,\n",
       " 0.002593759,\n",
       " 0.0018818273,\n",
       " 0.0024670146,\n",
       " 0.002494011,\n",
       " 0.0013066966,\n",
       " 0.0022469559,\n",
       " 0.0054901903,\n",
       " 0.0033042643,\n",
       " 0.0038988413,\n",
       " 0.00054510677,\n",
       " 0.0029637145,\n",
       " 0.001921086,\n",
       " 0.0016817586,\n",
       " 0.0034157743,\n",
       " 0.012646366,\n",
       " 0.0011995609,\n",
       " 0.0016777229,\n",
       " 0.002845083,\n",
       " 0.0016777966,\n",
       " 0.0024376258,\n",
       " 0.005264839,\n",
       " 0.0030219846,\n",
       " 0.0014979632,\n",
       " 0.0005337054,\n",
       " 0.0009070666,\n",
       " 0.0024408835,\n",
       " 0.0009347259,\n",
       " 0.0018679583,\n",
       " 0.0029511233,\n",
       " 0.0017351661,\n",
       " 0.003530104,\n",
       " 0.0021493994,\n",
       " 0.002740424,\n",
       " 0.0014159998,\n",
       " 0.0024625333,\n",
       " 0.0031723203,\n",
       " 0.0010688662,\n",
       " 0.0050851367,\n",
       " 0.0038750598,\n",
       " 0.0028481137,\n",
       " 0.0008910592,\n",
       " 0.0020308401,\n",
       " 0.0010681002,\n",
       " 0.0050264923,\n",
       " 0.0024179895,\n",
       " 0.00032173636,\n",
       " 0.0029663108,\n",
       " 0.002042345,\n",
       " 0.00291194,\n",
       " 0.001178743,\n",
       " 0.0037842728,\n",
       " 0.0011015206,\n",
       " 0.0018490881,\n",
       " 0.0027186105,\n",
       " 0.002852693,\n",
       " 0.00035813995,\n",
       " 0.0032457132,\n",
       " 0.0047216937,\n",
       " 0.0041730963,\n",
       " 0.0022262577,\n",
       " 0.0020086633,\n",
       " 0.005245417,\n",
       " 0.0020174012,\n",
       " 0.0018963282,\n",
       " 0.0018392397,\n",
       " 0.00032333436,\n",
       " 0.0016252958,\n",
       " 0.0030716695,\n",
       " 0.0023449103,\n",
       " 0.0050200606,\n",
       " 0.006705773,\n",
       " 0.0012924052,\n",
       " 0.0014675056,\n",
       " 0.00089312,\n",
       " 0.0015398646,\n",
       " 0.0029790283,\n",
       " 0.0076113464,\n",
       " 0.001588363,\n",
       " 0.0055584824,\n",
       " 0.0028040337,\n",
       " 0.0035534587,\n",
       " 0.0016795059,\n",
       " 0.000490646,\n",
       " 0.002901558,\n",
       " 0.0011823387,\n",
       " 0.0037269462,\n",
       " 0.0013519565,\n",
       " 0.002692355,\n",
       " 0.0075726924,\n",
       " 0.003023903,\n",
       " 0.0023625055,\n",
       " 0.0015473658,\n",
       " 0.0021664659,\n",
       " 0.0016563252,\n",
       " 0.0023766938,\n",
       " 0.0030999847,\n",
       " 0.0037036221,\n",
       " 0.00031320177,\n",
       " 0.0019360343,\n",
       " 0.0023478346,\n",
       " 0.0016156316,\n",
       " 0.0026409351,\n",
       " 0.0023287407,\n",
       " 0.002115788,\n",
       " ...]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = (x_train / 255).astype(\"float32\")\n",
    "decodedImages = autoencoder.predict(images)\n",
    "\n",
    "errors = []\n",
    "for i in range(0, len(images)):\n",
    "    errors.append(np.mean((images[i] - decodedImages[i]) ** 2))\n",
    "    \n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  440,   635,   670,   910,  1015,  1111,  1165,  1310,  1489,\n",
       "        1495,  1501,  1590,  1666,  1677,  1758,  1820,  2302,  2532,\n",
       "        2671,  2693,  2842,  2845,  2901,  3094,  3290,  3354,  3416,\n",
       "        3512,  3590,  3679,  3682,  3906,  3943,  3968,  4058,  4066,\n",
       "        4216,  4265,  4270,  4279,  4289,  4347,  4355,  4362,  4502,\n",
       "        4729,  4827,  4834,  5009,  5108,  5113,  5135,  5398,  5422,\n",
       "        6084,  6444,  6758,  6814,  6857,  6967,  7018,  7171,  7184,\n",
       "        7203,  7207,  7231,  7254,  7281,  7337,  7375,  7642,  7807,\n",
       "        8016,  8047,  8050,  8675,  8687,  8690,  8826,  9042,  9195,\n",
       "        9386,  9455,  9505,  9521,  9552,  9834,  9863,  9966,  9994,\n",
       "       10026, 10032, 10176, 10204, 10218, 10251, 10677, 11028, 11133,\n",
       "       11181, 11191, 11409, 11428, 11482, 11584, 11734, 11848, 12188,\n",
       "       12268, 12327, 12339, 12614, 12785, 12848, 12891, 13076, 13238,\n",
       "       13588, 13622, 13719, 13781, 13831, 13912, 13926, 14334, 14362,\n",
       "       14406, 14481, 14629, 14728, 14937, 15017, 15169, 15382, 15504,\n",
       "       15734, 15783, 15988, 16103, 16192, 16275, 16430, 16552, 16598,\n",
       "       16621, 16634, 16748, 16777, 16817, 16836, 16894, 17128, 17200,\n",
       "       17314, 17426, 17466, 17521, 17553, 17592, 17593, 17603, 17679,\n",
       "       17958, 18147, 18219, 18398, 18431, 18504, 18579, 18585, 18590,\n",
       "       18631, 18685, 18687, 18701, 18737, 18799, 18832, 18869, 19362,\n",
       "       19373, 19612, 19686, 19954, 20016, 20175, 20254, 20283, 20355,\n",
       "       20357, 20609, 20724, 21299, 21686, 21688, 21766, 21855, 21874,\n",
       "       21960, 21996, 22008, 22021, 22103, 22130, 22294, 22308, 22378,\n",
       "       22436, 22440, 22445, 22634, 23042, 23136, 23273, 23582, 23586,\n",
       "       23629, 23733, 23794, 23831, 23837, 23868, 23884, 24134, 24480,\n",
       "       24598, 24728, 24794, 25137, 25312, 25801, 25942, 25986, 26085,\n",
       "       26095, 26195, 26309, 26728, 26753, 26778, 26834, 27017, 27029,\n",
       "       27112, 27118, 27120, 27145, 27162, 27247, 27263, 27417, 27661,\n",
       "       28030, 28160, 28242, 28428, 28540, 28556, 28778, 28898, 29048,\n",
       "       29052, 29055, 29056, 29121, 29148, 29172, 29187, 29199, 29237,\n",
       "       29345, 29560, 29664, 29695, 29991, 30041, 30248, 30255, 30455,\n",
       "       30604, 30800, 30865, 30870, 30889, 31043, 31070, 31079, 31497,\n",
       "       31530, 31577, 31631, 31661, 31673, 31683, 31688, 31719, 31844,\n",
       "       31962, 32040, 32086, 32203, 32221, 32372, 32399, 32711, 32776,\n",
       "       33208, 33412, 33522, 33770, 34500, 34514, 34574, 34608, 34623,\n",
       "       34639, 34678, 34839, 34845, 34942, 35325, 35494, 35697, 35712,\n",
       "       35843, 35934, 35947, 35962, 36072, 36214, 36560, 36628, 36730,\n",
       "       36866, 36884, 37027, 37287, 37303, 37373, 37382, 37425, 37529,\n",
       "       37541, 37543, 37554, 37571, 37667, 37738, 37750, 37946, 38042,\n",
       "       38102, 38143, 38171, 38197, 38355, 38679, 39008, 39031, 39039,\n",
       "       39318, 39321, 39389, 39474, 39561, 39874, 40082, 40149, 40376,\n",
       "       40378, 40396, 40726, 40829, 41101, 41211, 41331, 41358, 41578,\n",
       "       41627, 41634, 41789, 41795, 41882, 42042, 42063, 42096, 42224,\n",
       "       42442, 42479, 42526, 42580, 42827, 42890, 42934, 42943, 42965,\n",
       "       42973, 43068, 43072, 43088, 43422, 43452, 43930, 43986, 44011,\n",
       "       44442, 44456, 44590, 44598, 44652, 44709, 44753, 44796, 44841,\n",
       "       44848, 44870, 44875, 44881, 44917, 44944, 44951, 44955, 45115,\n",
       "       45274, 45439, 45467, 45676, 45728, 45755, 45763, 45779, 45919,\n",
       "       45942, 46011, 46070, 46104, 46114, 46131, 46179, 46377, 46516,\n",
       "       46560, 46579, 46701, 46796, 46944, 46959, 47260, 47324, 47383,\n",
       "       47389, 47438, 47451, 47531, 47597, 47607, 47615, 47690, 47723,\n",
       "       47736, 47781, 47965, 48037, 48176, 48203, 48350, 48360, 48371,\n",
       "       48403, 48419, 48472, 48532, 48594, 48638, 48796, 48895, 48911,\n",
       "       48929, 48947, 48973, 48997, 49067, 49073, 49119, 49135, 49153,\n",
       "       49163, 49244, 49268, 49300, 49438, 49500, 49535, 49537, 49637,\n",
       "       49819, 49883, 49889, 50018, 50054, 50061, 50192, 50390, 50393,\n",
       "       50434, 50457, 50463, 50542, 50584, 50826, 51014, 51176, 51323,\n",
       "       51415, 51459, 51649, 52014, 52114, 52273, 52326, 52396, 52804,\n",
       "       52817, 52862, 52933, 53036, 53085, 53090, 53403, 53407, 53970,\n",
       "       53996, 54085, 54089, 54117, 54162, 54600, 54728, 54832, 55285,\n",
       "       55330, 55395, 55438, 55496, 55503, 55616, 55804, 55890, 56175,\n",
       "       56191, 56239, 56569, 56573, 56659, 56662, 56842, 56848, 56891,\n",
       "       56968, 57000, 57020, 57262, 57408, 57434, 57532, 57705, 57707,\n",
       "       57831, 57837, 57898, 57907, 57985, 58038, 58160, 58170, 58237,\n",
       "       58275, 58356, 58362, 58371, 58386, 58394, 58424, 58445, 58801,\n",
       "       58829, 58845, 58871, 58872, 58897, 58908, 59059, 59076, 59202,\n",
       "       59312, 59420, 59439, 59447, 59496, 59712], dtype=int64)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = np.quantile(errors, 0.99)\n",
    "outliers = np.where(errors >= thresh)[0]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    encodedData = []\n",
    "    for image in data:\n",
    "        img = (np.expand_dims(image, axis=-1) / 255.0).astype(\"float32\")\n",
    "        encodedData.append(img)\n",
    "    return np.array(encodedData)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
